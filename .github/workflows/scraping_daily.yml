name: Daily Selenium Scraping

on:
  schedule:
    - cron: '20 2 * * *'
  workflow_dispatch:  # Permite ejecutar manualmente el flujo si es necesario

jobs:
  scraping:
    runs-on: ubuntu-latest

    steps:
    # Paso 1: Clonar el código del repositorio
    - name: Checkout repository
      uses: actions/checkout@v2

    # Paso 2: Configurar Python
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.8'

    # Paso 3: Instalar dependencias necesarias para Selenium y otros módulos
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install selenium
        pip install webdriver-manager

    # Paso 4: Ejecutar el script de scraping con el archivo test.csv
    - name: Run Selenium scraper with test CSV
      run: python inpres_sismos/inpres_sismos/selenium/actualizar_sismos.py

    # Paso 5: Mostrar el contenido del CSV para verificar si se actualizó
    - name: Mostrar contenido del CSV
      run: cat data/test.csv

    # Paso 6: Agregar los cambios del CSV
    - name: Add changes to git
      run: |
        git config --global user.name "github-actions"
        git config --global user.email "github-actions@github.com"
        git add data/test.csv
        git commit -m "Actualizar test.csv con nuevos datos de sismos"

    # Paso 7: Hacer push de los cambios al repositorio
    - name: Push changes to repository
      run: |
        git push https://github.com/${{ github.repository }} HEAD:${{ github.ref }}
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}